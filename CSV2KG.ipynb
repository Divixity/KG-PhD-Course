{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea173702c9b135f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "General approach:\n",
    "1. For each PK cell in the table, get the top 3 entities from Wikidata.\n",
    "2. For each other cell in the table, get the relations and objects for each of the top 3 entities.\n",
    "3. For each column, get the most common relation.\n",
    "4. For each column, get the object that has the most common relation with the PK cell.\n",
    "5. For the PK column, get the most common type.\n",
    "\n",
    "The code is a bit messy with a lot of loop, however, it does work. The results are fine, but there sure are improvements to be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-30T13:40:50.864890Z",
     "start_time": "2024-04-30T13:40:50.847912Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from lookup import WikidataAPI\n",
    "from endpoints import WikidataEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "acc50a9af218fdfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:30:13.821899Z",
     "start_time": "2024-04-29T08:30:13.805363Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query to return matches for object and relation given value from a QID\n",
    "def get_relation_object_query(QID, value):\n",
    "    if isinstance(value, str):\n",
    "        filter_string = f'FILTER(?object = \"{value}\").'\n",
    "    else:\n",
    "        filter_string = f'FILTER(?object = {value}).'\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT ?relation ?object\n",
    "    WHERE {{\n",
    "      {{\n",
    "        wd:{QID} ?relation ?object.\n",
    "        ?object rdfs:label \"{value}\"@en.\n",
    "      }}\n",
    "      UNION\n",
    "      {{\n",
    "        wd:{QID} ?relation ?object.\n",
    "        {filter_string}\n",
    "      }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "\n",
    "class CSV2KG:\n",
    "    def __init__(self, df):\n",
    "        self.wikidata = WikidataAPI()\n",
    "        self.ep = WikidataEndpoint()\n",
    "        self.df = df\n",
    "        self.df_candidates = None\n",
    "        self.df_relations = None\n",
    "        self.df_entities = None\n",
    "        self.df_classes = None\n",
    "        \n",
    "    def get_candidates(self):\n",
    "        dataframe = pd.DataFrame(columns=self.df.columns, dtype=object)#self.df.copy()\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            search_string = row.iloc[0]\n",
    "            \n",
    "            # Entities\n",
    "            entities = self.wikidata.getKGEntities(query=search_string, \n",
    "                                      limit=3, \n",
    "                                      type=\"item\")\n",
    "            \n",
    "            dataframe.at[idx, self.df.columns[0]] = entities\n",
    "            \n",
    "                \n",
    "            for search_idx in row.index[1:]:\n",
    "                search_obj_dict = {}\n",
    "                search_obj = row[search_idx]\n",
    "                \n",
    "                if search_obj == search_obj:  # Check if it is not NaN\n",
    "                \n",
    "                    for entity in entities:\n",
    "                        QID = entity.getId().split(\"/\")[-1]\n",
    "                        \n",
    "                        query = get_relation_object_query(QID, search_obj)\n",
    "                        results = self.ep.getQueryResults(query)\n",
    "                        \n",
    "                        rel_obj_list = []\n",
    "                        if len(results[\"results\"][\"bindings\"]) > 0:\n",
    "                            for res in results[\"results\"][\"bindings\"]:\n",
    "                                rel = res[\"relation\"][\"value\"]\n",
    "                                obj = res[\"object\"][\"value\"]\n",
    "                                rel_obj_list.append((rel, obj))\n",
    "                                \n",
    "                        search_obj_dict[entity.getId()] = rel_obj_list\n",
    "\n",
    "                dataframe.at[idx, search_idx] = search_obj_dict\n",
    "        \n",
    "        self.df_candidates = dataframe\n",
    "    \n",
    "    \n",
    "    def get_relations(self):\n",
    "        counts = {}\n",
    "        for col in self.df_candidates.columns[1:]:\n",
    "            relation_counts = {}\n",
    "            for idx, row in self.df_candidates.iterrows():\n",
    "                for entity, search_obj_dict in row[col].items():\n",
    "                    for rel, obj in search_obj_dict:\n",
    "                        relation_counts[rel] = relation_counts.get(rel, 0) + 1\n",
    "            counts[col] = relation_counts\n",
    "            \n",
    "        self.df_relations = pd.DataFrame(columns=self.df.columns)\n",
    "        for col, rel_counts in counts.items():\n",
    "            if rel_counts:\n",
    "                relation = max(rel_counts, key=rel_counts.get)        \n",
    "                self.df_relations.at[0, col] = relation\n",
    "            else:\n",
    "                self.df_relations.at[0, col] = None\n",
    "            \n",
    "    def get_entities(self):\n",
    "        self.df_entities = pd.DataFrame(columns=self.df.columns, index=self.df.index)\n",
    "        for idx, row in self.df_candidates.iterrows():\n",
    "            for col in self.df_candidates.columns[1:]:\n",
    "                for entity, search_obj_dict in row[col].items():\n",
    "                    if self.df_relations.at[0, col] in [rel for rel, obj in search_obj_dict]:\n",
    "                        if self.df_entities.at[idx, self.df_entities.columns[0]] != self.df_entities.at[idx, self.df_entities.columns[0]]:\n",
    "                            self.df_entities.at[idx, self.df_entities.columns[0]] = entity\n",
    "        \n",
    "        for idx, row in self.df_entities.iterrows():\n",
    "            for col in self.df_entities.columns[1:]:\n",
    "                subject = self.df_entities.at[idx, self.df_entities.columns[0]]\n",
    "                \n",
    "                if subject == subject:\n",
    "                    for prop, obj in self.df_candidates.at[idx, col].get(subject, []):\n",
    "                        if prop == self.df_relations.at[0, col]:\n",
    "                            self.df_entities.at[idx, col] = obj\n",
    "                            \n",
    "    def get_types(self):\n",
    "        self.df_classes = pd.DataFrame(columns=self.df.columns)\n",
    "        \n",
    "        types_dct = {}\n",
    "        for idx, row in self.df_candidates.iterrows():\n",
    "            entites = row.iloc[0]\n",
    "            for entity in entites:\n",
    "                types = self.ep.getTypesForEntity(entity.getId())\n",
    "                for t in types:\n",
    "                    types_dct[t] = types_dct.get(t, 0) + 1\n",
    "        \n",
    "        if types_dct:\n",
    "            type_entity = max(types_dct, key=types_dct.get)\n",
    "        else:\n",
    "            type_entity = None\n",
    "        self.df_classes.at[0, self.df_classes.columns[0]] = type_entity\n",
    "        \n",
    "        for col in self.df_entities.columns[1:]:\n",
    "            types_dct = {}\n",
    "            for idx, row in self.df_entities.iterrows():\n",
    "                entity = row[col]\n",
    "                if entity == entity:\n",
    "                    types = self.ep.getTypesForEntity(entity.split(\"/\")[-1])\n",
    "                    for t in types:\n",
    "                        types_dct[t] = types_dct.get(t, 0) + 1\n",
    "            \n",
    "            if types_dct:\n",
    "                type_entity = max(types_dct, key=types_dct.get)\n",
    "                self.df_classes.at[0, col] = type_entity    \n",
    "        \n",
    "    \n",
    "    def run_alignment(self):\n",
    "        self.get_candidates()\n",
    "        self.get_relations()\n",
    "        self.get_entities()\n",
    "        self.get_types()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ecb69179fc3aba",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Run for 5 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1cd0cb83b69f7a87",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-29T08:30:13.823332Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download the data from https://github.com/sem-tab-challenge/2024/blob/main/data/WikidataTables2024R1.tar.gz\n",
    "path_to_tables = './data/WikidataTables2024R1/DataSets/Valid/tables/'\n",
    "file_names = os.listdir(path_to_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9aebc345d26ff58c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.730432Z",
     "start_time": "2024-04-29T08:30:13.825280Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03ENJ6XM.csv\n",
      "08IZY6G9.csv\n",
      "0CZW7M0F.csv\n",
      "0DYQDSCY.csv\n",
      "0ER7T81U.csv\n",
      "0JGLUKC7.csv\n",
      "0JLTIHOL.csv\n",
      "0LBWW5L4.csv\n",
      "0N0NEYH9.csv\n",
      "0N5SQTLA.csv\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for file_name in file_names[:10]:\n",
    "    df = pd.read_csv(path_to_tables + file_name)\n",
    "    mappings = CSV2KG(df)\n",
    "    mappings.run_alignment()\n",
    "    results.append([file_name, mappings])\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e4ceb82be5b603",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CEA (Cell-Entity Annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c1744ff83479e239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.767468Z",
     "start_time": "2024-04-29T08:31:06.742184Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cea = pd.read_csv(\"./data/WikidataTables2024R1/DataSets/Valid/gt/cea_gt.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9df96585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03ENJ6XM.csv\n",
      "  Correct: 2, Incorrect: 3\n",
      "  Precision: 1.0,  Recall: 0.4,  F1: 0.5714285714285715\n",
      "08IZY6G9.csv\n",
      "  Correct: 15, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0CZW7M0F.csv\n",
      "  Correct: 2, Incorrect: 1\n",
      "  Precision: 1.0,  Recall: 0.6666666666666666,  F1: 0.8\n",
      "0DYQDSCY.csv\n",
      "  Correct: 0, Incorrect: 10\n",
      "  Precision: 0,  Recall: 0.0,  F1: 0\n",
      "0ER7T81U.csv\n",
      "  Correct: 6, Incorrect: 2\n",
      "  Precision: 1.0,  Recall: 0.75,  F1: 0.8571428571428571\n",
      "0JGLUKC7.csv\n",
      "  Correct: 2, Incorrect: 5\n",
      "  Precision: 1.0,  Recall: 0.2857142857142857,  F1: 0.4444444444444445\n",
      "0JLTIHOL.csv\n",
      "  Correct: 0, Incorrect: 4\n",
      "  Precision: 0,  Recall: 0.0,  F1: 0\n",
      "0LBWW5L4.csv\n",
      "  Correct: 0, Incorrect: 8\n",
      "  Precision: 0,  Recall: 0.0,  F1: 0\n",
      "0N0NEYH9.csv\n",
      "  Correct: 6, Incorrect: 4\n",
      "  Precision: 1.0,  Recall: 0.6,  F1: 0.7499999999999999\n",
      "0N5SQTLA.csv\n",
      "  Correct: 0, Incorrect: 8\n",
      "  Precision: 0,  Recall: 0.0,  F1: 0\n",
      "Average Precision: 0.6, Average Recall: 0.3702380952380952, Average F1: 0.4423015873015873\n"
     ]
    }
   ],
   "source": [
    "avg_precision = 0\n",
    "avg_recall = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "for res_no in range(len(results)):\n",
    "    # These variables are used to calculate the precision, recall and F1 score\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    subset_df = cea[cea[0] == results[res_no][0].removesuffix('.csv')]\n",
    "    for idx, row in subset_df.iterrows():\n",
    "        mapping = results[res_no][1].df_entities.iloc[row[1] - 1, row[2]]\n",
    "        if mapping == row[3]:  # If we found the correct mapping, increment the true positives\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            if mapping != mapping:  # If we didn't find a mapping, increment the false negatives\n",
    "                false_negatives += 1\n",
    "            else:  # If we found a mapping, but it was incorrect, increment the false positives\n",
    "                false_positives += 1\n",
    "\n",
    "    print(f\"{results[res_no][0]}\\n  Correct: {true_positives}, Incorrect: {false_negatives + false_positives}\")\n",
    "\n",
    "    try:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "    avg_precision += precision\n",
    "\n",
    "    try:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "    avg_recall += recall\n",
    "\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    avg_f1 += f1\n",
    "    \n",
    "    print(f\"  Precision: {precision},  Recall: {recall},  F1: {f1}\")\n",
    "\n",
    "print(f\"Average Precision: {avg_precision / len(results)}, Average Recall: {avg_recall / len(results)}, Average F1: {avg_f1 / len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb460737e7e38147",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CTA (Column-Type Annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "eec16fe923fdbdc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.795007Z",
     "start_time": "2024-04-29T08:31:06.772410Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cta = pd.read_csv(\"./data/WikidataTables2024R1/DataSets/Valid/gt/cta_gt.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e91e7c76ef024a9e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.797141Z",
     "start_time": "2024-04-29T08:31:06.779812Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>03ENJ6XM</td>\n",
       "      <td>0</td>\n",
       "      <td>http://www.wikidata.org/entity/Q54050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0  1                                      2\n",
       "392  03ENJ6XM  0  http://www.wikidata.org/entity/Q54050"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cta[cta[0] == results[0][0].removesuffix('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "eb73ca4f1034fd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.832287Z",
     "start_time": "2024-04-29T08:31:06.790692Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q207326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     col0 col1 col2\n",
       "0  http://www.wikidata.org/entity/Q207326  NaN  NaN"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0][1].df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a9dbb36a157fa048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.867566Z",
     "start_time": "2024-04-29T08:31:06.798022Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03ENJ6XM.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "08IZY6G9.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0CZW7M0F.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0DYQDSCY.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "0ER7T81U.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0JGLUKC7.csv\n",
      "  Correct: 0, Incorrect: 2\n",
      "  Precision: 0.0,  Recall: 0.0,  F1: 0\n",
      "0JLTIHOL.csv\n",
      "  Correct: 0, Incorrect: 2\n",
      "  Precision: 0.0,  Recall: 0.0,  F1: 0\n",
      "0LBWW5L4.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "0N0NEYH9.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0N5SQTLA.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "Average Precision: 0.4, Average Recall: 0.4, Average F1: 0.4\n"
     ]
    }
   ],
   "source": [
    "avg_precision = 0\n",
    "avg_recall = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "for res_no in range(len(results)):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    subset_df = cta[cta[0] == results[res_no][0].removesuffix('.csv')]\n",
    "    \n",
    "    for idx, row in subset_df.iterrows():\n",
    "        mapping = results[res_no][1].df_classes.iloc[0, row[1]]\n",
    "\n",
    "        if mapping == row[2]:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            if mapping != mapping:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "    print(f\"{results[res_no][0]}\\n  Correct: {true_positives}, Incorrect: {false_negatives + false_positives}\")\n",
    "\n",
    "    try:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "    avg_precision += precision\n",
    "\n",
    "    try:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "    avg_recall += recall\n",
    "\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    avg_f1 += f1\n",
    "    \n",
    "    print(f\"  Precision: {precision},  Recall: {recall},  F1: {f1}\")\n",
    "\n",
    "print(f\"Average Precision: {avg_precision / len(results)}, Average Recall: {avg_recall / len(results)}, Average F1: {avg_f1 / len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28baf8c99ebfde17",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CPA (Columns-Property Annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "869b779464d9b60c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:06.868910Z",
     "start_time": "2024-04-29T08:31:06.800270Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpa = pd.read_csv(\"./data/WikidataTables2024R1/DataSets/Valid/gt/cpa_gt.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "39d5f3dc800a3263",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T08:31:07.219424Z",
     "start_time": "2024-04-29T08:31:06.808175Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03ENJ6XM.csv\n",
      "  Correct: 2, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "08IZY6G9.csv\n",
      "  Correct: 2, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0CZW7M0F.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0DYQDSCY.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "0ER7T81U.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0JGLUKC7.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0JLTIHOL.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "0LBWW5L4.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "0N0NEYH9.csv\n",
      "  Correct: 1, Incorrect: 0\n",
      "  Precision: 1.0,  Recall: 1.0,  F1: 1.0\n",
      "0N5SQTLA.csv\n",
      "  Correct: 0, Incorrect: 1\n",
      "  Precision: 0.0,  Recall: 0,  F1: 0\n",
      "Average Precision: 0.6, Average Recall: 0.6, Average F1: 0.6\n"
     ]
    }
   ],
   "source": [
    "avg_precision = 0\n",
    "avg_recall = 0\n",
    "avg_f1 = 0\n",
    "\n",
    "for res_no in range(len(results)):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    subset_df = cpa[cpa[0] == results[res_no][0].removesuffix('.csv')]\n",
    "    \n",
    "    for idx, row in subset_df.iterrows():\n",
    "        mapping = results[res_no][1].df_relations.iloc[row[1], row[2]]\n",
    "        \n",
    "        if mapping == row[3]:\n",
    "            true_positives += 1\n",
    "        else:\n",
    "            if mapping != mapping:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "    print(f\"{results[res_no][0]}\\n  Correct: {true_positives}, Incorrect: {false_negatives + false_positives}\")\n",
    "\n",
    "    try:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "    except ZeroDivisionError:\n",
    "        precision = 0\n",
    "    avg_precision += precision\n",
    "\n",
    "    try:\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    except ZeroDivisionError:\n",
    "        recall = 0\n",
    "    avg_recall += recall\n",
    "\n",
    "    try:\n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    except ZeroDivisionError:\n",
    "        f1 = 0\n",
    "    avg_f1 += f1\n",
    "    \n",
    "    print(f\"  Precision: {precision},  Recall: {recall},  F1: {f1}\")\n",
    "\n",
    "print(f\"Average Precision: {avg_precision / len(results)}, Average Recall: {avg_recall / len(results)}, Average F1: {avg_f1 / len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dc4709e59c22c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-29T08:31:06.953089Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
